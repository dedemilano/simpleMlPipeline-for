{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cca0b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The Alley feature has too many missing values, so we will drop it from the dataset.\n",
    "\n",
    "##### Based on the data, YearSold and OverallCondition are only weakly correlated with SalePrice. While we could consider removing them, we should proceed with caution, as they might interact with other variables in ways that reveal a stronger relationship.\n",
    "\n",
    "##### The Street feature may not be useful since it contains only two categories, and one of them occurs very rarely.\n",
    "\n",
    "##### According to the metadata, we should not include the Foundation feature in the model, so it will be excluded.\n",
    "\n",
    "##### The CentralAir feature has a strong class imbalance, so I will not include it in the model.\n",
    "\n",
    "##### Although the SaleType feature also shows class imbalance, it includes a wide variety of values. For now, I will keep it.\n",
    "\n",
    "##### In the SaleCondition column, there are two identical values written differently: \"Normal\" and \"normal\". We will unify them by using \"Normal\" to maintain consistency in capitalization.\n",
    "\n",
    "##### The LotArea feature contains too many unique values, so I will group them into bins.\n",
    "\n",
    "##### Similarly, the GrLivArea feature has a high number of unique values, so I will also bin it.\n",
    "\n",
    "##### For TotalBsmtSF, I will consider creating bins to group the values into ranges.\n",
    "\n",
    "##### I will explore combining FullBath and HalfBath into a single feature, as they both represent bathrooms and appear to be related.\n",
    "\n",
    "##### I will create bins for GarageCars\n",
    "\n",
    "##### I will also create bins for GarageArea, and evaluate the correlation between GarageCars and GarageArea. If they are too closely related, one of them may be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dcbbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(path='../data/dataset.csv'):\n",
    "    \"\"\"Load the housing dataset.\"\"\"\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce67221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import features\n",
    "\n",
    "\n",
    "def preprocess_data(df, save_path=None, version=None , features_to_drop = [\n",
    "        'Foundation',  # excluded based on metadata\n",
    "        'CentralAir',  # strong class imbalance\n",
    "        'OverallCondition',  # weakly correlated with SalePrice\n",
    "        'Street',  # only two categories with one occurring rarely\n",
    "        'Alley',  # too many missing values\n",
    "        'BldgType',\n",
    "        'HouseStyle',\n",
    "        'GarageType',\n",
    "        'SaleType',\n",
    "        'SaleCondition',\n",
    "        'LotType',\n",
    "        'GarageArea',\n",
    "        'SaleAge',\n",
    "    ]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for the housing dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: The input DataFrame\n",
    "        save_path: Optional path to save the processed data\n",
    "        version: Optional version number to append to saved files\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Processed data splits\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Target variable\n",
    "    target = 'SalePrice'\n",
    "    \n",
    "    # Step 1: Handle missing values\n",
    "    data['Alley'].fillna('Missing', inplace=True)\n",
    "    data['GarageType'].fillna('Missing', inplace=True)\n",
    "    data['GarageArea'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Step 2: Feature engineering\n",
    "    # Age-related features\n",
    "    data['HouseAge'] = 2025 - data['YearBuilt']\n",
    "    data['SaleAge'] = 2025 - data['YearSold']\n",
    "    data.drop(columns=['YearBuilt', 'YearSold'], inplace=True)\n",
    "    \n",
    "    # Bathroom feature\n",
    "    data['TotalBath'] = data['FullBath'] + (0.5 * data['HalfBath'])\n",
    "    data.drop(columns=['FullBath', 'HalfBath'], inplace=True)\n",
    "    \n",
    "    # Step 3: Group rare values in categorical features\n",
    "    # LotType grouping\n",
    "    if 'LotType' in data.columns:\n",
    "        data[\"LotType\"] = data[\"LotType\"].replace({'FR2': 'FR', 'FR3': 'FR'})\n",
    "    \n",
    "    # GarageType grouping\n",
    "    if 'GarageType' in data.columns:\n",
    "        data['GarageType'] = data['GarageType'].replace({\n",
    "            'Basment': 'Other', \n",
    "            'CarPort': 'Other', \n",
    "            '2Types': 'Other'\n",
    "        })\n",
    "    \n",
    "    # SaleType grouping\n",
    "    if 'SaleType' in data.columns:\n",
    "        data['SaleType'] = data['SaleType'].replace({\n",
    "            'ConLD': 'Other', \n",
    "            'ConLI': 'Other', \n",
    "            'ConLw': 'Other', \n",
    "            'CWD': 'Other', \n",
    "            'Oth': 'Other', \n",
    "            'Con': 'Other'\n",
    "        })\n",
    "    \n",
    "    # SaleCondition grouping and standardization\n",
    "    if 'SaleCondition' in data.columns:\n",
    "        data['SaleCondition'] = data['SaleCondition'].replace({'normal': 'Normal'})\n",
    "        data['SaleCondition'] = data['SaleCondition'].replace({\n",
    "            'Partial': 'Other',\n",
    "            'Abnorml': 'Other',\n",
    "            'Family': 'Other',\n",
    "            'Alloca': 'Other',\n",
    "            'AdjLand': 'Other'\n",
    "        })\n",
    "    \n",
    "    # Step 4: Define features to drop (excluded during preprocessing)\n",
    "    \n",
    "    # Step 5: Separate features and target (WITHOUT dropping columns)\n",
    "    y = data[target]\n",
    "    X = data.drop(columns=[target])\n",
    "\n",
    "    features_to_drop = features_to_drop\n",
    "    \n",
    "    # Step 6: Identify numerical and categorical columns (EXCLUDING features_to_drop)\n",
    "    numerical_features = [\n",
    "        col for col in X.select_dtypes(include=['int64', 'float64']).columns \n",
    "        if col not in features_to_drop\n",
    "    ]\n",
    "    print(f\"Numerical features: {numerical_features}\")\n",
    "    categorical_features = [\n",
    "        col for col in X.select_dtypes(include=['object']).columns \n",
    "        if col not in features_to_drop\n",
    "    ]\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    # Step 7: Create preprocessing pipelines\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    transformers = [('num', numerical_transformer, numerical_features)]\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "    \n",
    "    # Step 8: Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 9: Apply preprocessing (automatically drops unprocessed columns)\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Step 10: Get feature names after transformation\n",
    "    feature_names = numerical_features.copy()\n",
    "    if categorical_features:\n",
    "        categorical_features_out = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "        feature_names += list(categorical_features_out)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "    \n",
    "    # Step 11: Save processed data if requested\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        suffix = f\"_{version}\" if version else \"\"\n",
    "        X_train_processed.to_csv(os.path.join(save_path, f'X_train{suffix}.csv'), index=False)\n",
    "        X_test_processed.to_csv(os.path.join(save_path, f'X_test{suffix}.csv'), index=False)\n",
    "        pd.DataFrame(y_train).to_csv(os.path.join(save_path, f'y_train{suffix}.csv'), index=False)\n",
    "        pd.DataFrame(y_test).to_csv(os.path.join(save_path, f'y_test{suffix}.csv'), index=False)\n",
    "    \n",
    "    return X_train_processed, X_test_processed, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0274f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_path = os.path.join('..', 'data', 'dataset.csv')\n",
    "# data = load_data(data_path)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# X_train, X_test, y_train, y_test = preprocess_data(data, save_path='../processed_data', version=1  )\n",
    "\n",
    "# # Print shapes to verify\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0df273",
   "metadata": {},
   "source": [
    "### for the second training version and the followings we add more features according to the correlation matrix seen in the EDA phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8515ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we add GarageType ( Remember we should not work with Foundation )\n",
    "# data_path = os.path.join('..', 'data', 'dataset.csv')\n",
    "# data = load_data(data_path)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# X_train, X_test, y_train, y_test = preprocess_data(data, save_path='../processed_data', version=2 ,features_to_drop=[\n",
    "#         'Foundation',  # excluded based on metadata\n",
    "#         'CentralAir',  # strong class imbalance\n",
    "#         'OverallCondition',  # weakly correlated with SalePrice\n",
    "#         'Street',  # only two categories with one occurring rarely\n",
    "#         'Alley',  # too many missing values\n",
    "#         'BldgType',\n",
    "#         'HouseStyle',\n",
    "#         'SaleType',\n",
    "#         'SaleCondition',\n",
    "#         'LotType',\n",
    "#         'GarageArea',\n",
    "#         'SaleAge',\n",
    "#     ])\n",
    "\n",
    "# # Print shapes to verify\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399d0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### we add SaleType for the 3rd version ( Remember we should not work with Foundation )\n",
    "# data_path = os.path.join('..', 'data', 'dataset.csv')\n",
    "# data = load_data(data_path)\n",
    "\n",
    "# # Apply preprocessing\n",
    "# X_train, X_test, y_train, y_test = preprocess_data(data, save_path='../processed_data', version=3 ,features_to_drop=[\n",
    "#         'Foundation',  # excluded based on metadata\n",
    "#         'CentralAir',  # strong class imbalance\n",
    "#         'OverallCondition',  # weakly correlated with SalePrice\n",
    "#         'Street',  # only two categories with one occurring rarely\n",
    "#         'Alley',  # too many missing values\n",
    "#         'BldgType',\n",
    "#         'HouseStyle',\n",
    "#         'SaleCondition',\n",
    "#         'LotType',\n",
    "#         'GarageArea',\n",
    "#         'SaleAge',\n",
    "#     ])\n",
    "\n",
    "# # Print shapes to verify\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667f0166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: ['LotArea', 'GrLivArea', 'OverallQuality', 'OverallCondition', 'TotalBsmtSF', 'GarageCars', 'HouseAge', 'TotalBath']\n",
      "Categorical features: ['HouseStyle', 'GarageType', 'SaleType', 'SaleCondition']\n",
      "X_train shape: (1168, 24)\n",
      "X_test shape: (292, 24)\n",
      "y_train shape: (1168,)\n",
      "y_test shape: (292,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gueid\\AppData\\Local\\Temp\\ipykernel_30684\\114837799.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Alley'].fillna('Missing', inplace=True)\n",
      "C:\\Users\\gueid\\AppData\\Local\\Temp\\ipykernel_30684\\114837799.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['GarageType'].fillna('Missing', inplace=True)\n",
      "C:\\Users\\gueid\\AppData\\Local\\Temp\\ipykernel_30684\\114837799.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['GarageArea'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "### we add SaleCondition , HouseStyle and OverallCondition for the 4th version ( Remember we should not work with Foundation )\n",
    "data_path = os.path.join('..', 'data', 'dataset.csv')\n",
    "data = load_data(data_path)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train, X_test, y_train, y_test = preprocess_data(data, save_path='../processed_data', version=4 ,features_to_drop=[\n",
    "        'Foundation',  # excluded based on metadata\n",
    "        'CentralAir',  # strong class imbalance\n",
    "        'Street',  # only two categories with one occurring rarely\n",
    "        'Alley',  # too many missing values\n",
    "        'BldgType',\n",
    "        'LotType',\n",
    "        'GarageArea',\n",
    "        'SaleAge',\n",
    "    ])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
